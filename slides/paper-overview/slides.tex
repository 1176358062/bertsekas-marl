\documentclass{beamer}

\mode<presentation> {


%\usetheme{Darmstadt}
%\usetheme{Dresden}
    \usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}


\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{hyperref}

\usepackage{graphicx} % Allows including images
\graphicspath{ {images/} }
\usepackage{xcolor}




\AtBeginSection[]{
	\begin{frame}
		\vfill
		\centering
		\begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
			\usebeamerfont{title}\insertsectionhead\par%
		\end{beamercolorbox}
		\vfill
	\end{frame}
}



%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[MARL]{Multiagent Reinforcement Learning:
Rollout and Policy Iteration
    (Bertsekas, 2020)}
\author{Mikalai Korbit} % Your name

\institute[IMT] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
    IMT School for Advanced Studies Lucca %\\ % Your institution for the title page
%\medskip
%\textit{todo@outlook.com} % Your email address
}
\date{\today} % Date, can be changed to a custom date

\begin{document}

    \begin{frame}
        \titlepage % Print the title page as the first slide
    \end{frame}

    \begin{frame}
        \frametitle{Outline} % Table of contents slide, comment this block out to remove it
        \tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
    \end{frame}


%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------


%------------------------------------------------


    \section{MARL Overview}
%------------------------------------------------




    \begin{frame}
        \frametitle{Motivation}

        \begin{block}{Multi-agent systems are ubiquitous}
            Eg. fleet of drones, factory robots, self-driving cars.
        \end{block}

        \begin{block}{Recent advances in RL applications}
            Eg. AlphaGo/AlphaZero, playing Starcraft, robotic control.
        \end{block}

        \begin{block}{Utilize modern computer architecture and software frameworks}
            Eg. cloud computing, stacks of graphics cards, TPUs;
            PyTorch, OpenAI gyms.
        \end{block}

        \begin{block}{Benefits of modeling a problem as MARL}
            Scalability, robustness, faster learning through experience sharing,
            parallel computation.
        \end{block}

    \end{frame}



%------------------------------------------------


    \begin{frame}
        \frametitle{Multi-Agent Reinforcement Learning Problem}
        Inherits Reinforcement Learning characteristics:
        \begin{itemize}
            \item Learning how to map situations into actions
            \item Trial-and-error search
            \item Delayed feedback
            \item Trade-off between exploration and exploitation
            \item Sequential decision making
            \item Agent's actions affect the subsequent data it receives
        \end{itemize}

        Adds multi-agent features:
        \begin{itemize}
            \item Actions of one agent influence other agents' rewards
            \item Communication problem
            \item Fully cooperative, fully info sharing (DP) vs. partial info sharing
            \item Curse of dimensionality (more severe than in RL)
        \end{itemize}

    \end{frame}


%------------------------------------------------

\begin{frame}
	\frametitle{``Bertsekas Dictionary''}
	
	Aligns optimal control definitions with the RL-world:
	\begin{itemize}
		\item Maximize value $\rightarrow$ minimize cost		
		\item Agent $\rightarrow$ Decision maker or controller
		\item Action $\rightarrow$ Decision or control
		\item Environment  $\rightarrow$ Dynamic system
		\item Learning $\rightarrow$ Solving a DP-related problem using simulation
		\item Self-learning (self-play) $\rightarrow$ Solving a DP problem using simulation-based policy iteration
		\item Planning vs Learning distinction $\rightarrow$ Solving a DP problem with model-based vs model-free simulation	
	\end{itemize}
	
\end{frame}


%------------------------------------------------


    \begin{frame}
        \frametitle{Multi-Agent MDP}

        \begin{figure}
            \includegraphics[scale=0.65]{1a_marl}
            \caption{MARL Problem. Source: Sadhu, Konar (2020)}
        \end{figure}

        \begin{itemize}
            \item All agents see the global state $s$
            \item Individual actions: $u^{a} \in U$
            \item State transitions: $P\left(s^{\prime} \mid s, \mathbf{u}\right): S \times \mathbf{U} \times S \rightarrow[0,1]$
            \item Shared team reward: $S \times \mathbf{U} \rightarrow \mathbb{R}$ 
            
            
        \end{itemize}

    \end{frame}


%------------------------------------------------


    \begin{frame}
        \frametitle{Taxonomy}

        \begin{block}{Cooperative}
            \begin{itemize}
                \item The goal of cooperative agents is to achieve a common objective
                \item Coordination problem
            \end{itemize}
        \end{block}

        \begin{block}{Competitive}
            \begin{itemize}
                \item Zero-sum games (eg. chess, tic-tac-toe)
                \item Minimax equilibria
            \end{itemize}
        \end{block}

        \begin{block}{Mixed}
            \begin{itemize}
                \item  General-sum games (win-win, lose-lose scenarios;
                eg. pollution model, ``what movie to watch?'')
                \item  Nash equilibria
            \end{itemize}
        \end{block}

    \end{frame}

%------------------------------------------------


    \begin{frame}
        \frametitle{First Attempts to Solve MARL}

        \begin{block}{Independent Q-Learning (Tan 93)}
            \begin{itemize}
                \item Each agent learns its own Q-function
                \item Each agent treats the other agents as part of the environment
                \item Independent Q-learning update rule for agent $a$:
                $$
                Q\left(s_{t}, u_{t}^{a}\right) \leftarrow Q\left(s_{t}, u_{t}^{a}\right)+\alpha\left[r_{t}+\gamma \max _{u} Q\left(s_{t+1}, u\right)-Q\left(s_{t}, u_{t}^{a}\right)\right]
                $$
                
            \end{itemize}

        \end{block}

        \begin{block}{Coordinated Q-Learning (Guestrin et al. 02)}
            \begin{itemize}
                \item Stationary learning of joint value function 
                $Q_{t o t}(s, \mathbf{u})$
                \item Factorize to improve scalability:
                $$
                Q_{t o t}(s, \mathbf{u})=\sum_{e=1}^{E} Q_{e}\left(s^{e}, \mathbf{u}^{e}\right)
                $$

            \end{itemize}
        \end{block}

    \end{frame}



























%------------------------------------------------


    \section{Multiagent Rollout}
%------------------------------------------------


    \begin{frame}
	\frametitle{Key Ideas}
	
	
        \begin{block}{Deal with the exponential increase in the action space}
	$\rightarrow$ Introduce a form of sequential
	agent-by-agent one-step lookahead minimization --
	\textit{multiagent rollout}
	
\end{block}

\begin{block}{Compute the agent actions in parallel}
	$\rightarrow$ Decouple sequential 
	agent-by-agent computation with \textit{precomputed
		signaling policy} that embodies agent coordination
\end{block}	
	
	

	\end{frame}


%------------------------------------------------



    \begin{frame}
        \frametitle{The Setting}
        
        \begin{itemize}
			\item $P2_F$ -- stochastic discrete-time optimal control problem over a finite horizon, with perfect information on the state
			\item Fully cooperative
			\item Tested in Spiders-And-Flies environment
		\end{itemize}
	
	$$x_{k+1}=f_{k}\left(x_{k}, u_{k}, w_{k}\right), \quad k=0,1, \ldots, N-1$$
	
	$$J_{\pi}\left(x_{0}\right)=E\left\{g_{N}\left(x_{N}\right)+\sum_{k=0}^{N-1} g_{k}\left(x_{k}, \mu_{k}\left(x_{k}\right), w_{k}\right)\right\}$$
	
	
    \end{frame}


%------------------------------------------------


    \begin{frame}
	\frametitle{Policy Iteration and Rollout}
	
    \begin{figure}
		\includegraphics[scale=0.65]{2a_pi}
		\caption{Policy Iteration Algorithm. Source: Bertsekas (2020)}
	\end{figure}
	
	
	\begin{itemize}
		\item Fundamental property: policy improvement
		$$J_{k, \tilde{\pi}}\left(x_{k}\right) \leq J_{k, \pi}\left(x_{k}\right), \quad \forall x_{k}, k$$
		
	\end{itemize}
	
	\end{frame}

%------------------------------------------------

\begin{frame}
	\frametitle{Standard Rollout Algorithm}
	
	\begin{itemize}
	\item Rollout is one-time policy iteration
	\item Start with the initial
	state $x0$, and generate a trajectory:
	
	$$
	\left\{x_{0}, \tilde{u}_{0}, x_{1}, \tilde{u}_{1}, \ldots, x_{N-1}, \tilde{u}_{N-1}, x_{N}\right\}
	$$
	
	Where  $\tilde{u}_{k}$ is 
		
	$$
	\begin{aligned}
	\tilde{u}_{k} \in \arg \min _{u_{k} \in U_{k}\left(x_{k}\right)} E\left\{g_{k}\left(x_{k}, u_{k}, w_{k}\right) + \right.\\
	&\left.+J_{k+1, \pi}\left(f_{k}\left(x_{k}, u_{k}, w_{k}\right)\right)\right\}
	\end{aligned}
	$$
	
	\item Defines rollout policy that 
	possesses \textbf{cost improvement property}
	and \textbf{robustness property} (can adapt to 
	changes in data distributions online)
	
	\item Works when argmin over small set of $U$!
	
	
\end{itemize}
	
	
\end{frame}



%------------------------------------------------


\begin{frame}
	\frametitle{Standard Rollout for MA Case (All-at-once Rollout)}
	
	\begin{itemize}
		\item The control constraint set becomes 
		the Cartesian product

$$
U_{k}\left(x_{k}\right)=U_{k}^{1}\left(x_{k}\right) \times \cdots \times U_{k}^{m}\left(x_{k}\right)
$$

		\item Argmin is now computed over $q^m$! (where $q$ is an upper bound to the number of
		controls in $U_{k}$, $m$ is the number of agents)

		\item Idea: trade-off control space complexity 
		with state space complexity

	\end{itemize}
	
\end{frame}

%------------------------------------------------





\begin{frame}
	\frametitle{One-at-a-time Rollout (Multiagent Rollout)}
	
    \begin{figure}
	\includegraphics[scale=0.4]{2b}
	\caption{One-at-a-time action selection. Source: Bertsekas (2020)}
	\end{figure}	
	
		
	\begin{enumerate}
		\item Break down $u_k$ into the sequence of $m$ actions: 
		$u^1_k, u^2_k, ..., u^m_k$
		
		\item Introduce artificial states $\left(x_{k}, u_{k}^{1}\right),\left(x_{k}, u_{k}^{1}, u_{k}^{2}\right), \ldots,\left(x_{k}, u_{k}^{1}, \ldots, u_{k}^{m-1}\right)$
		\item $u^m_k$ marks the transition to the new state 
		$x_{k+1}=f(x_k, u_k, w_k)$ incurring cost $g_k(x_k, u_k, w_k)$
	\end{enumerate}


	
\end{frame}


%------------------------------------------------

    \begin{frame}
	\frametitle{Benefits of the Multiagent Rollout Algorithm}
	
			Past controls determined by
		the rollout policy, and the future controls determined by
		the base policy!
	
	
	
	\begin{itemize}
		\item Reducing the action space by increasing the state space. Reasonable since Q-factor minimization is performed for just
		one state at each stage. 	
		
		\item We reduce the computation complexity 
		from $O(q^m)$ to $O(qm)$, $q=|U|$
		
		\item In addition to that, solves coordination.
		problem.
		
		\item Preserves \textbf{cost improvement property} 
		(see Bertsekas, part II.D 
		for proof by induction for $m=2$).
		
	\end{itemize}
	
	
	\end{frame}



%------------------------------------------------
    \begin{frame}
	\frametitle{Multiagent Rollout Assumptions}
	
	\begin{enumerate}
		\item All agents have access to the current state $x_k$;
		
		\item There is an order in which agents compute and apply
		their local controls;
		
		\item There is ``intercommunication'' between agents, 
		so that agent $l$ knows the local controls 
		$u_k^1, u_k^2, ..., u_k^{l-1}$ 
		computed by the
		predecessor agents 	$1, 2, ..., l-1$ 
		in the given order.
	\end{enumerate}
	
	
\end{frame}




%------------------------------------------------

%------------------------------------------------
    \begin{frame}
    	
	\frametitle{Ordering of Agents}
	
	
	
	\begin{itemize}
		\item Instead of predefined or random order, 
		at each step $k$ optimize over single 
		agent's Q-factors.
		
		\item Simulate $m$ sequences where each agent 
		acts first, select the one with minimal Q-factor,
		``compete'' for the second place with $m-1$ agents,
		etc.
				 
		\item Total number of minimizations:
		$$
		m+(m-1)+\cdots+1=\frac{m(m+1)}{2}
		$$
		
		 
		\item Computations can be parallelized.
	\end{itemize}
	
	
	\end{frame}













%------------------------------------------------
    \section{Extensions}
%------------------------------------------------



    \begin{frame}
	\frametitle{Approximate Policy Iteration with Agent-by-Agent Policy Improvement}
	
	\begin{figure}
		\includegraphics[scale=0.55]{3a_api}
		\caption{Approximate Policy Iteration. Source: Bertsekas (2020)}
	\end{figure}
	
	
	\begin{itemize}
		\item Approximate policy improvement property: With approximations, policy improvement holds approximately
		\item If a single policy iteration is done (rollout), no need to train value and policy networks
		\item Multiple policy iterations can be done only with off-line training
	
	\end{itemize}
\end{frame}


%------------------------------------------------


    \begin{frame}
        \frametitle{Parallel Computation of Agents' Controls}
        \begin{itemize}
            \item Reminder: parallel computation vs. action coordination.
                        
            \item First Attempt: since the agent $l$
            does not know the 
            rollout controls for the agents $1, ..., l − 1$, 
            uses
            the controls $\mu_{k}^{1}\left(x_{k}\right), \ldots, \mu_{k}^{\ell-1}\left(x_{k}\right)$ of the base policy in their
            place.
            
            \item Drawback: does not preserve cost improvement property. 
                        
        \end{itemize}
    \end{frame}


%------------------------------------------------

\begin{frame}
	\frametitle{Autonomous Multiagent Rollout}
	\begin{itemize}
		\item Second Attempt: assume that once the agents know the
		state, they use precomputed approximations to the control
		components of the preceding agents, and compute their own
		control components in parallel and asynchronously -- 
		\textbf{autonomous multiagent rollout}.
		
		
		\item How to compute approximations? Train a neural network 
		off-line training  with training samples generated
		through the rollout policy -- \textbf{signaling policy}.
		
				
		\item Use base and signaling policies 
		to generate a rollout policy 
		$\tilde{\pi}=\left\{\tilde{\mu}_{0}, \ldots, \tilde{\mu}_{N-1}\right\}$
		autonomously in parallel.
		
	\end{itemize}
\end{frame}

%------------------------------------------------

\begin{frame}
\frametitle{Autonomous Multiagent Rollout Mechanics}

$$
\begin{array}{c}
\tilde{\mu}_{k}^{1}\left(x_{k}\right) \in \arg \min _{u_{k}^{1} \in U_{k}^{1}\left(x_{k}\right)} E\left\{g_{k}\left(x_{k}, u_{k}^{1}, \mu_{k}^{2}\left(x_{k}\right),\right.\right. \\
\left.\ldots, \mu_{k}^{m}\left(x_{k}\right), w_{k}\right) \\
+J_{k+1, \pi}\left(f_{k}\left(x_{k}, u_{k}^{1}, \mu_{k}^{2}\left(x_{k}\right)\right.\right. \\
\left.\left.\left.\ldots, \mu_{k}^{m}\left(x_{k}\right), w_{k}\right)\right)\right\} \\
\tilde{\mu}_{k}^{2}\left(x_{k}\right) \in \arg \min _{u_{k}^{2} \in U_{k}^{2}\left(x_{k}\right)} E\left\{g_{k}\left(x_{k}, \widehat{\mu}_{k}^{1}\left(x_{k}\right), u_{k}^{2}\right.\right. \\
\left.\ldots, \mu_{k}^{m}\left(x_{k}\right), w_{k}\right) \\
+J_{k+1, \pi}\left(f_{k}\left(x_{k}, \widehat{\mu}_{k}^{1}\left(x_{k}\right), u_{k}^{2}\right.\right. \\
\left.\left.\left.\ldots, \mu_{k}^{m}\left(x_{k}\right), w_{k}\right)\right)\right\} \\
\ldots & \ldots \\
\qquad \begin{array}{c}
\mu_{k}^{m}\left(x_{k}\right) \in \arg \min _{u_{k}^{m} \in U_{k}^{m}\left(x_{k}\right)} E\left\{g_{k}\left(x_{k}, \widehat{\mu}_{k}^{1}\left(x_{k}\right)\right.\right. \\
\left.\ldots, \widehat{\mu}_{k}^{m-1}\left(x_{k}\right), u_{k}^{m}, w_{k}\right)
\end{array} \\
+J_{k+1, \pi}\left(f_{k}\left(x_{k}, \widehat{\mu}_{k}^{1}\left(x_{k}\right)\right.\right. \\
\left.\left.\left.\ldots, \widehat{\mu}_{k}^{m-1}\left(x_{k}\right), u_{k}^{m}, w_{k}\right)\right)\right\}
\end{array}
$$

\end{frame}

%------------------------------------------------



\begin{frame}
\frametitle{Synchronized Autonomous Multiagent Rollout}
	
	\begin{itemize}
		\item What if we allow periodic updates 
		of the signaling policies?
		
	\end{itemize}
	
	
\begin{figure}
	\includegraphics[scale=0.65]{3b_idea}
\end{figure}



\end{frame}



%------------------------------------------------
%------------------------------------------------
%------------------------------------------------

    \section{Conclusion}

    \begin{frame}
        \frametitle{Conclusion}
        \begin{itemize}
            \item MARL problems are especially prone to the
            curse of the dimensionality problem;
            
            \item We could reduce the action space 
            by allowin agent-be-agent updates;

            \item We could parallelize computations
            by adding a signaling policy (precalculated offline);
            
            \item Multi-agent rollout can be extended with 
            approximate policy iteration;

            
        \end{itemize}
    \end{frame}


    \begin{frame}
        \frametitle{References}
        \footnotesize{
            \begin{thebibliography}{10}

                \bibitem{bert20}\label{bert20}
                Dimitri Bertsekas -- Multiagent Reinforcement Learning: Rollout and Policy Iteration (2020). Web:
                \url{https://web.mit.edu/dimitrib/www/Multiagent_Sinica_2020.pdf}

                \bibitem{whiteson20}\label{whiteson20}
                Shimon Whiteson -- Multi-Agent Reinforcement Learning Reinforcement (July 2019) [Eastern European Machine Learning Summer School Seminar].


                \bibitem{sadhu20}\label{sadhu20}
                Arup Kumar Sadhu, Amit Konar --
                Multi-Agent Coordination,
                A Reinforcement Learning Approach (2020).


            \end{thebibliography}
        }
    \end{frame}

%------------------------------------------------

    \begin{frame}

        \begin{center}
            \Huge Thanks for
            \\
            your attention!
        \end{center}

    \end{frame}

%----------------------------------------------------------------------------------------

\end{document}